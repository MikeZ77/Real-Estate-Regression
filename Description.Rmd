---
output:
  pdf_document: default
  html_document: default
---
# A Step-By-Step Guide for Running a Complete Multiple Linear Regression Analysis in R

The point of this guide is to give new data scientists a step-by-step approach running a complete multiple linear regression (MLR) analysis without needing a deep background in statistics. Just note that in order to get the most out of this material, you should be familiar with concepts of OLS (Ordinary Least Squares) for SLR (Simple Linear Regression). One issue I have found with many data science tutorials on the topic of regression is that the approach is incomplete. Only fitting a model and evaluating the R^2 is a simplified approach that is problematic. If certain model assumptions (more on this later) are not met: (1) This may result in biased estimators (do not minimize the variance) and as a result we will be left with a model that does not provide the best possible fit (leading to relatively worse prediction). (2) If we want to construct confidence intervals or hypothesis tests on our regression, these may be invalid. 

Obviously, this detailed type of analysis is outside the scope of many of these courses (many of which assume no prior knowledge), but they should at the very least mention that there is more to it. If you are building a SLR or MLR model of substance that could be used for real decision making, you will want to complete the full analysis. My goal for this guide is to hopefully allow you to bridge the gap between an introductory tutorial on regression and a full semester undergraduate course on the topic. At each step, I try and explain the theory (when necessary), concepts, and possible difficulties and pain points that may arise in your own data set. I recommend R for regression analysis over Python due to its simplicity (one-liners) and ease is plotting the required graphs. That being said, the same analysis can be completed in Python using a library like statsmodels. I break the analysis into five separate steps:

1. A brief description of the data
2. Checking for multicollinearity
3. Performing a risidual analysis
4. Variable selection
5. Model validation

For this guide, I am using a sample data set suitable for regression analysis available from Kaggle called Real Estate Price Predication (https://www.kaggle.com/quantbruce/real-estate-price-prediction). For a domain like this, we are looking to predict and build confidence intervals around the price of a house.

# A Brief Description of the Data

You should already be familiar with the process of going through descriptive statistics in order better understand the data. However, there is one very useful plot for regression analysis that should be incorporated. This is the pairs plot.  

```{r}
library(data.table)

# First read in the data
df <- read.csv("real_estate.csv")
df$No <- NULL

# Quick cleanup, rename the column headers
setnames(df, 
         old = c('X1.transaction.date',
                 'X2.house.age', 
                 'X3.distance.to.the.nearest.MRT.station', 
                 'X4.number.of.convenience.stores',
                 'X5.latitude',
                 'X6.longitude',
                 'Y.house.price.of.unit.area'), 
         new = c('Transaction',
                 'Age',
                 'Station',
                 'Stores',
                 'Latitude',
                 'Longitude',
                 'Price')
         )

#Pairs plot
pairs(df[,c(2:7)])

```

The pairs plot summarizes the relationships between all our variables. If you haven't encountered a pairs plot before, all you have to do is start at the diagonal, and line up the intersection between any two variables. You only need to look at the pairs on one side of the diagonal, since the other side will be mirrored. Through the pairs plot, we can get general feeling for:

1. Any colinearity between predictors
2. Predictors that have a weak relationship and may be phased out in variable selection
3. The general strength of the relationship between the response and the predictors

In our example, we can tell that there is probably some overall linear relationship between price and the predictors. The number of stores appears to have the strongest relationship, so we can expect it to remain in the model and have a significant linear relationship. If you get a data set that shows no relationship on that bottom row, and you are building a model for predication, now might be a good time to stop and move on to some data that is more useful. There are some interesting relationships between latitude (lat), longitude (lon), and the station (distance). One likely possibility is that the lat and lon near the mean have a low distance to MRT stations because those coordinates are in the city center. It is not clear whether this will cause an issue for the model. However, if you see near diagonal set of data points for any two predictors, multicollinearity will definitely be an issue that should be resolved.


```{r}

write.csv(df,'real_estate2.csv', row.names = FALSE)

```


